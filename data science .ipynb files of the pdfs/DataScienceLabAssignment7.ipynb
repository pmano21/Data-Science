{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataScienceLabAssignment7.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Q1> Implement a Simple Neural Network Model and predict if a person will buy insurance or not\n","for the given dataset. [Excel Uploaded in Google Classroom]"],"metadata":{"id":"8NESpmRSxdmC"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded=files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"id":"Q0VV2UYyCXsf","outputId":"9cf94c44-c282-46ca-d054-a114fcc420cc","executionInfo":{"status":"error","timestamp":1649174779462,"user_tz":-330,"elapsed":2885,"user":{"displayName":"puligiri moses","userId":"07862833137557037810"}}},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-e0218664-7c11-495c-b0e4-d9fc902c577b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-e0218664-7c11-495c-b0e4-d9fc902c577b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"error","ename":"MessageError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-49432676dcaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   \"\"\"\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    121\u001b[0m   result = _output.eval_js(\n\u001b[1;32m    122\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m--> 123\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m    124\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read properties of undefined (reading '_uploadFiles')"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2HUYpfkCHDW","executionInfo":{"status":"aborted","timestamp":1649174504218,"user_tz":-330,"elapsed":54,"user":{"displayName":"puligiri moses","userId":"07862833137557037810"}}},"outputs":[],"source":["import keras\n","from keras.models import Sequential   # For building the Neural Network layer by layer\n","from keras.layers import Dense        # used to add fully connected layer in ANN#Simple Neural Network Model Code using Tensorflow\n","import pandas as pd                   # import pandas and numpy for datafram and numpy operations\n","import numpy as np\n","import matplotlib.pyplot as plt       # import matplotlib for graphical visualizations \n","import io                             # import io for reading csv files\n","from sklearn.preprocessing import StandardScaler      # for standardizing the data\n","from sklearn.metrics import accuracy_score            # to determine the accuracy score\n","\n","## importing the dataset\n","df= pd.read_csv('insurance_data.csv')           \n","print(df.head())\n","\n","n=df.shape[1]               # n--> number of columns\n","m=df.shape[0]               # m--> number of rows\n","\n","x_train=np.array(df.iloc[:int(0.9*m),:n-1])      # we take 90% of rows in training data\n","y_train=np.array(df.iloc[:int(0.9*m),-1])        # only the last column is in y_train  \n","x_test=np.array(df.iloc[int(0.9*m):,:n-1])       # only 10 % of rows is in testing dataset\n","y_test=np.array(df.iloc[int(0.9*m):,-1])         # splitting into training and testing dataset \n","\n","\n","\n","## here bought_insurance is already encoded as it can be a binary number\n","sc = StandardScaler()\n","x_train_scaled= sc.fit_transform(x_train)    # here we standardize the training and testing data using standardscaler()\n","x_test_scaled=sc.fit_transform(x_test)       # we standardize them so that they are easy to compute\n","\n","model=Sequential()                                      # we create our neural network model here\n","model.add(Dense(2, input_dim=2, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function\n","model.add(Dense(2, input_dim=2, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function \n","model.add(Dense(1, activation='softmax'))               # finally the output dimension has only 1 parameter(0 or 1) so we use softmax activation function  \n","                                                        # output dimension has 1 parameter as output\n","\n","#\"compile\" is a method of Tensorflow. “adam’ is the optimizer that can perform the gradient descent.\n","# loss function as binary_crossentropy as we have 2 cases only 0 or 1\n","# The optimizer updates the weights during training and reduces the loss.                                                        \n","model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n","\n","model.fit(x_train_scaled, y_train, epochs=50,batch_size=25) # we fit our model using this\n","model.evaluate(x_test_scaled,y_test)                        # we evaluate using our scaled testing data\n","y_pred=model.predict(x_test_scaled)                         # we predict based on scaled testing data\n","actual_res=y_test.tolist()                                  # actual_res contains the actual result list  \n","pred_res=y_pred.tolist()                                    # pred_res contains the predicted result list\n","\n","\n","a = accuracy_score(pred_res,actual_res)                     # we compute the accuracy based on accuracy_score function from sklearn.metrics\n","print('Accuracy is:', a*100)                                # final accuracy result "]},{"cell_type":"markdown","source":["Q2> Use the given dataset and build a customer churn prediction model using artificial neural network.\n","[Concept used: ANN Classification, Use Multiple Hidden Layers, Confusion Matrix to check the\n","accuracy rate of the model]\n","Dataset : kaggle.com/datasets/mltuts/churn-modelling-data?select=Churn_Modelling.csv"],"metadata":{"id":"mVrUDzDAxjyQ"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded=files.upload()"],"metadata":{"id":"lVE56XW0yHw5","executionInfo":{"status":"aborted","timestamp":1649174504222,"user_tz":-330,"elapsed":57,"user":{"displayName":"puligiri moses","userId":"07862833137557037810"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keras\n","from keras.models import Sequential   # For building the Neural Network layer by layer\n","from keras.layers import Dense        # used to add fully connected layer in ANN#Simple Neural Network Model Code using Tensorflow\n","import pandas as pd                   # import pandas and numpy for datafram and numpy operations\n","import numpy as np\n","import matplotlib.pyplot as plt       # import matplotlib for graphical visualizations \n","import io                             # import io for reading csv files\n","from sklearn.preprocessing import StandardScaler      # for standardizing the data\n","from sklearn.metrics import accuracy_score            # to determine the accuracy score\n","from sklearn.preprocessing import OneHotEncoder\n","\n","## importing the dataset\n","df= pd.read_csv('Churn_Modelling.csv')           \n","print(df.head())\n","\n","print(\"The dataset after dopping first 3 columns are : \")\n","df.drop(df.columns[[0,1,2]],axis=1,inplace=True)    # we drop few columns because we dont need them \n","print(df.head())\n","n=df.shape[1]               # n--> number of columns\n","m=df.shape[0]               # m--> number of rows\n","\n","\n","def texttonum(vec):\n","  visited=[]\n","  res=[]\n","  index=0\n","  for i in vec:\n","    if(i in visited):\n","      res.append(visited.index(i))\n","    else:\n","      visited.append(i)\n","      res.append(index)\n","      index=index+1\n","  return res\n","df.iloc[:,1]=np.array(texttonum(df.iloc[:,1].tolist()))        # converting the text data in geography and gender column into categorical data\n","df.iloc[:,2]=np.array(texttonum(df.iloc[:,2].tolist()))        # we make 0 --> female 1--> male\n","print(df.head())\n","\n","x_train=np.array(df.iloc[:int(0.8*m),:n-1])      # we take 80% of rows in training data\n","y_train=np.array(df.iloc[:int(0.8*m),-1])        # only the last column is in y_train  \n","x_test=np.array(df.iloc[int(0.8*m):,:n-1])       # only 20 % of rows is in testing dataset\n","y_test=np.array(df.iloc[int(0.8*m):,-1])         # splitting into training and testing dataset\n","\n","\n","\n","sc = StandardScaler()\n","x_train_scaled= sc.fit_transform(x_train)    # here we standardize the training and testing data using standardscaler()\n","x_test_scaled=sc.fit_transform(x_test)       # we standardize them so that they are easy to compute\n","\n","\n","model=Sequential()                                      # we create our neural network model here\n","model.add(Dense(10, input_dim=10, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function\n","model.add(Dense(8, input_dim=10, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function \n","model.add(Dense(6, input_dim=8, activation='relu'))     # input dimensions to this layer is 2 and we use reLU activation function \n","model.add(Dense(1, activation='sigmoid'))               # finally the output dimension has only 1 parameter(0 or 1) so we use sigmoid activation function  \n","                                                        # output dimension has 1 parameter as output\n","\n","#\"compile\" is a method of Tensorflow. “adam’ is the optimizer that can perform the gradient descent.\n","# loss function as binary_crossentropy as we have 2 cases only 0 or 1\n","# The optimizer updates the weights during training and reduces the loss.                                                        \n","model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n","\n","model.fit(x_train_scaled, y_train, epochs=50,batch_size=250) # we fit our model using this\n","model.evaluate(x_test_scaled,y_test)                        # we evaluate using our scaled testing data\n","y_pred=model.predict(x_test_scaled)                         # we predict based on scaled testing data\n","actual_res=y_test.tolist()                                  # actual_res contains the actual result list  \n","pred_res=y_pred.tolist()                                    # pred_res contains the predicted result list\n","\n","for i in range(len(pred_res)):                    # since we used sigmoid function as our last activation layer\n","  if(pred_res[i][0]>=0.5):                        # we convert those values into either 0 or 1 based on whether they are greater than 0.5 or not\n","    pred_res[i]=1                                 # if greater or equal to 0.5 we make it 1\n","  else:                                            \n","    pred_res[i]=0                                 # if less than 0.5 we make it equal to 0 \n","\n","a = accuracy_score(pred_res,actual_res)                     # we compute the accuracy based on accuracy_score function from sklearn.metrics\n","print('Accuracy is:', a*100)                                # final accuracy result "],"metadata":{"id":"hf9rFB8Oy6ce","executionInfo":{"status":"aborted","timestamp":1649174504225,"user_tz":-330,"elapsed":58,"user":{"displayName":"puligiri moses","userId":"07862833137557037810"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CWAxyrZT_GJn","executionInfo":{"status":"aborted","timestamp":1649174504228,"user_tz":-330,"elapsed":60,"user":{"displayName":"puligiri moses","userId":"07862833137557037810"}}},"execution_count":null,"outputs":[]}]}